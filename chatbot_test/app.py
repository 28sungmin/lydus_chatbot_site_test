import os, pickle, numpy as np
from datetime import datetime
import tiktoken
from dotenv import load_dotenv, find_dotenv
import streamlit as st
import faiss
from openai import OpenAI
import re
from numpy.linalg import norm
import redis

import secrets
import streamlit.components.v1 as components
from streamlit_cookies_manager import EncryptedCookieManager
import time

#===================================================================================
# ì¿ í‚¤ ê´€ë¦¬(ë¡œê·¸ì¸ ê´€ë ¨)
#===================================================================================

# --- ì„¸íŒ…: originì€ ìŠ¬ë˜ì‹œ ì—†ì´! ---
APP_ORIGIN    = "https://lydus-chatbot.streamlit.app"   # ì´ Streamlit ì•± origin
PARENT_ORIGIN = "http://127.0.0.1:5500/index.html"     # ë¶€ëª¨ í˜ì´ì§€ origin
# -----------------------------------

cookies = EncryptedCookieManager(prefix="lydus_", password=st.secrets.get("COOKIE_PASSWORD","dev-pass"))
if not cookies.ready():
    st.stop()

# (A) ê¸°ì¡´ anon-* ê°’ì´ ë‚¨ì•„ ìˆìœ¼ë©´ 1íšŒ ì •ë¦¬
old = cookies.get("loginid")
print(old)
if old and str(old).startswith("anon-"):
    del cookies["loginid"]
    cookies.save()

# (B) JS: ë¶€ëª¨ì°½ì— loginid ìš”ì²­ -> ì‘ë‹µ ë°›ìœ¼ë©´ 1íšŒì„± ì¿¼ë¦¬ë¡œ ì „ë‹¬
components.html(f"""
<script>
(function(){{
  // ìì‹(ì´ íƒ­) -> ë¶€ëª¨ì—ê²Œ ìš”ì²­ : targetOriginì€ 'ë¶€ëª¨'!
  if (window.opener) {{
    window.opener.postMessage("REQUEST_LOGINID", "{PARENT_ORIGIN}");
  }}
  // ë¶€ëª¨ì˜ ì‘ë‹µ ìˆ˜ì‹  : e.originì€ 'ë¶€ëª¨'!
  window.addEventListener("message", function(e){{
    if (e.origin !== "{PARENT_ORIGIN}") return;
    var data = e.data || {{}};
    if (!data.loginid) return;

    // 1íšŒì„±ìœ¼ë¡œ URL ì¿¼ë¦¬ì— ì‹£ê³  ë¦¬ë¡œë“œ (ì„œë²„ íŒŒì´ì¬ì´ ECMì— ì €ì¥í•˜ë„ë¡)
    var u = new URL(window.location.href);
    u.searchParams.set("li", data.loginid);
    window.location.replace(u.toString());
  }});
}})();
</script>
""", height=0)

# (C) ì¿¼ë¦¬ ìˆ˜ì‹  ì‹œ ECMì— ì €ì¥í•˜ê³  URL ì •ë¦¬
qp = st.query_params
if "li" in qp:
    new_id = qp["li"]
    cookies["loginid"] = new_id
    cookies.save()
    # ì„¸ì…˜ì—ë„ ì¦‰ì‹œ ë°˜ì˜
    st.session_state["loginid"] = new_id
    # URL ì¿¼ë¦¬ ì œê±°
    st.query_params.clear()

# (D) ìµœì¢… ì‚¬ìš©: ì•ˆì „ ì ‘ê·¼ (KeyError ë°©ì§€)
loginid = st.session_state.get("loginid") or cookies.get("loginid")
if loginid:
    st.session_state["loginid"] = loginid   # ì„¸ì…˜ì— ê³ ì •
    st.success(f"ë¡œê·¸ì¸ ì•„ì´ë””: {loginid}")
else:
    st.info("ë¡œê·¸ì¸ ì•„ì´ë”” ìˆ˜ì‹  ì¤‘ì…ë‹ˆë‹¤. íŒì—… í—ˆìš© ë° ë„ë©”ì¸ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.")
#===================================================================================
# ì„¤ì •
#===================================================================================
load_dotenv(find_dotenv(), override=True)

api_key = os.environ.get('OPENAI_API_KEY')
client = OpenAI(api_key=api_key)

redis_host = os.environ.get("REDIS_HOST")
redis_port = int(os.environ.get("REDIS_PORT"))
redis_password = os.environ.get("REDIS_PASSWORD")

EMBED_MODEL     = "text-embedding-3-small"
CHAT_MODEL_GENERAL      = "gpt-4.1"
CHAT_MODEL_MINI      = "gpt-4o-mini"
TOP_K           = 4
r = redis.Redis(
    host=redis_host,
    port=redis_port,
    decode_responses=True,
    username="default",
    password=redis_password,
)
STOPWORDS = ["ì•Œë ¤", "ìˆ˜", "ìˆì–´", "ì–´ë””", "ë‚˜ì˜¤", "ëŠ”ì§€", "ì—ì„œ", "ìœ¼ë¡œ", "í•˜ê³ ", "ê°€ì´ë“œë¼ì¸", 'í™•ì¸', 'í™•ì¸í•˜ê³ ', 'ì‹¶ì–´', 'í˜ì´ì§€', 'ì–´ëŠ', 'ë¶€ë¶„']
TOKEN_LIMIT = 50 # í•˜ë£¨ í•œ ì‚¬ëŒë‹¹ 1000ì› -> 260000í† í°?
USER_ID = "user124"

# 1ê¶Œ
IDX_FILE_1        = "data/book1_faiss_chunk_250804.index"
META_FILE_1       = "data/book1_meta_chunk_250804.pkl"
SECTION_IDX_FILE_1 = "data/book1_faiss_section_keywords_250804.index"
SECTION_META_FILE_1 = "data/book1_meta_section_keywords_250804.pkl"
PAGE_IDX_FILE_1 = "data/book1_faiss_page_250804.index"
PAGE_META_FILE_1 = "data/book1_meta_page_250804.pkl"
# 2ê¶Œ
IDX_FILE_2        = "data/book2_faiss_chunk_250804.index"
META_FILE_2       = "data/book2_meta_chunk_250804.pkl"
SECTION_IDX_FILE_2 = "data/book2_faiss_section_keywords_250804.index"
SECTION_META_FILE_2 = "data/book2_meta_section_keywords_250804.pkl"
PAGE_IDX_FILE_2 = "data/book2_faiss_page_250804.index"
PAGE_META_FILE_2 = "data/book2_meta_page_250804.pkl"
# 3ê¶Œ
IDX_FILE_3        = "data/book3_faiss_chunk_250801.index"
META_FILE_3       = "data/book3_meta_chunk_250801.pkl"
SECTION_IDX_FILE_3 = "data/book3_faiss_section_keywords_250801.index"
SECTION_META_FILE_3 = "data/book3_meta_section_keywords_250801.pkl"
PAGE_IDX_FILE_3 = "data/book3_faiss_page_250801.index"
PAGE_META_FILE_3 = "data/book3_meta_page_250801.pkl"
# 4ê¶Œ
IDX_FILE_4        = "data/book4_faiss_chunk_table_250808.index"
META_FILE_4       = "data/book4_meta_chunk_table_250808.pkl"
SECTION_IDX_FILE_4 = "data/book4_faiss_section_keywords_250808.index"
SECTION_META_FILE_4 = "data/book4_meta_section_keywords_250808.pkl"
PAGE_IDX_FILE_4 = "data/book4_faiss_page_250808.index"
PAGE_META_FILE_4 = "data/book4_meta_page_250808.pkl"

with open(PAGE_META_FILE_1, "rb") as f:
    meta_pages_1 = pickle.load(f)
with open(SECTION_META_FILE_1, "rb") as f:
    meta_keywords_1 = pickle.load(f)
with open(META_FILE_1, "rb") as f:
    meta_chunks_1 = pickle.load(f)

with open(PAGE_META_FILE_2, "rb") as f:
    meta_pages_2 = pickle.load(f)
with open(SECTION_META_FILE_2, "rb") as f:
    meta_keywords_2 = pickle.load(f)
with open(META_FILE_2, "rb") as f:
    meta_chunks_2 = pickle.load(f)

with open(PAGE_META_FILE_3, "rb") as f:
    meta_pages_3 = pickle.load(f)
with open(SECTION_META_FILE_3, "rb") as f:
    meta_keywords_3 = pickle.load(f)
with open(META_FILE_3, "rb") as f:
    meta_chunks_3 = pickle.load(f)

with open(PAGE_META_FILE_4, "rb") as f:
    meta_pages_4 = pickle.load(f)
with open(SECTION_META_FILE_4, "rb") as f:
    meta_keywords_4 = pickle.load(f)
with open(META_FILE_4, "rb") as f:
    meta_chunks_4 = pickle.load(f)

PAGE_VOLUME_LIST = [("1ê¶Œ", meta_pages_1), ("2ê¶Œ", meta_pages_2), ("3ê¶Œ", meta_pages_3), ("4ê¶Œ", meta_pages_4)]
SECTION_VOLUME_LIST = [
    ("1ê¶Œ", meta_keywords_1),
    ("2ê¶Œ", meta_keywords_2),
    ("3ê¶Œ", meta_keywords_3),
    ("4ê¶Œ", meta_keywords_4),
]
#===================================================================================
# ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
#===================================================================================
def extract_keywords(question):
    prompt = (
        "From the question below, extract all the main subject, keyword, or technical term the user is asking about. "
        "Split all compound words and list every technical term separately, separated by commas. "
        "Do not group multiple terms together. "
        "Do not include any other words or explanation.\n\n"
        f"Question: {question}"
    )
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=20,
    )
    return [kw.strip() for kw in response.choices[0].message.content.strip().split(',')]

#===================================================================================
# ì§ˆë¬¸ì´ 1)ìœ„ì¹˜(ìŠˆë„ì½”ë“œ í¬í•¨) 2)ë‚´ìš© ì¸ì§€ë¥¼ íŒë‹¨í•˜ëŠ” í•¨ìˆ˜ë“¤
#===================================================================================
def is_location_question(question):
    keywords = ["ì–´ë””", "ì ˆ", "ìœ„ì¹˜", "ë‚˜ì™€", "í¬í•¨", "ì„¹ì…˜", "ë¶€ë¶„", "ë“¤ì–´ìˆ", "ì–¸ê¸‰", "í¬í•¨ëœ", "ìˆ˜ë¡"]
    return any(k in question for k in keywords)

def is_code_question(question):
    # 1ì°¨: ë‹¨ìˆœ í‚¤ì›Œë“œ ì²´í¬
    keywords = ["ìŠˆë„ì½”ë“œ", "ì½”ë“œ", "êµ¬í˜„"]
    if any(k in question for k in keywords):
        return True
    # 2ì°¨: ë‹¤ì–‘í•œ í‘œê¸°(ë„ì–´ì“°ê¸°, ì˜ì–´, ì˜¤íƒ€ ë“±) ì»¤ë²„
    if is_pseudocode(question):
        return True
    return False

def is_location_or_code_question_llm(question):
    prompt = (
        'If the following question is about the location of content '
        '(such as which section, part, where, included, mentioned, etc.) '
        'or about code, pseudocode, or implementation, answer YES. Otherwise, answer NO.\n\n'
        f'Q: {question}\n'
    )
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0,
        max_tokens=3,
    )
    answer = response.choices[0].message.content.strip().upper()
    return answer == "YES"

def classify_question(question):
    # 1. ì½”ë“œ/êµ¬í˜„ ê´€ë ¨ ì§ˆë¬¸ì¸ê°€?
    if is_code_question(question) or is_location_or_code_question_llm(question) == "YES":
        return "code"
    # 2. ìœ„ì¹˜ ê´€ë ¨ ì§ˆë¬¸ì¸ê°€?
    if is_location_question(question) or is_location_or_code_question_llm(question) == "NO":
        return "location"
    # 4. ê·¸ ì™¸ (ì„ë² ë”© ê²€ìƒ‰ ë“±)
    return "other"

#===================================================================================
# ì§ˆë¬¸ ì„ë² ë”© ë° í…ìŠ¤íŠ¸ ì„ë² ë”© íŒŒì¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ë“¤
#===================================================================================
def _embed_text(texts):
    resp = client.embeddings.create(model=EMBED_MODEL, input=texts)
    return [np.array(d.embedding, dtype="float32") for d in resp.data]

def build_or_load():
    loaded = []
    if os.path.exists(IDX_FILE_1) and os.path.exists(META_FILE_1):
        index_1 = faiss.read_index(IDX_FILE_1)
        loaded.append(("1ê¶Œ", index_1, meta_chunks_1))
    if os.path.exists(IDX_FILE_2) and os.path.exists(META_FILE_2):
        index_2 = faiss.read_index(IDX_FILE_2)
        loaded.append(("2ê¶Œ", index_2, meta_chunks_2))
    if os.path.exists(IDX_FILE_3) and os.path.exists(META_FILE_3):
        index_3 = faiss.read_index(IDX_FILE_3)
        loaded.append(("2ê¶Œ", index_3, meta_chunks_3))
    if os.path.exists(IDX_FILE_4) and os.path.exists(META_FILE_4):
        index_4 = faiss.read_index(IDX_FILE_4)
        loaded.append(("4ê¶Œ", index_4, meta_chunks_4))
    if not loaded:
        raise FileNotFoundError("ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return loaded

#===================================================================================
# ì§ˆë¬¸ ê²€ìƒ‰ ë° ë‹µë³€ ìƒì„± í•¨ìˆ˜ë“¤
#===================================================================================
def retrieve_multi_volume(query, top_k=4):
    q_emb = np.array(_embed_text([query])[0]).reshape(1, -1)
    results = []
    for label, index, meta in CHUNKS_VOLUME_LIST:
        D, I = index.search(q_emb, top_k)
        for idx in I[0]:
            chunk = meta[idx]
            # ì¶œì²˜ ì •ë³´ ì¶”ê°€ (ê¶Œ)
            chunk = dict(chunk)
            chunk["volume"] = label
            results.append(chunk)
    return results

def rag_chat_multi_volume(query, history, model=CHAT_MODEL_MINI):
    context_blobs = retrieve_multi_volume(query)
    # ê° ë¸”ë¡ì— ì¶œì²˜ í‘œì‹œ
    context_text = "\n\n".join(
        f"[{c['volume']}] {c['text']}" for c in context_blobs
    )

    messages = (
        [{"role": "system",
          "content": "Task: \n"
                     "You are a helpful RAG assistant. Given a user question and context, answer appropriately."

                     "Instructions: \n"
                     "1. Use only the provided context to answer the user's question."
                     "2. For the terms 'ì •ë°€ë„ (precision)' and 'ì •ë°€ì„± (preciseness)':"
                            "- Do NOT ever confuse or mix up these two terms."
                            "- Each term is a distinct metric with its own unique definition and formula."
                            "- If the question is about 'ì •ë°€ë„', only provide the definition and formula for precision."
                            "- If the question is about 'ì •ë°€ì„±', only provide the definition and formula for preciseness."
                     "3. For any formula or equation mentioned in the document:" 
                            "- Show it **exactly** as it appears in the text."    
                            "- Do not modify, rephrase, or re-typeset."
                            "- Just copy and paste the original LaTeX or expression as-is from the document."

                     "Output Format:\n"
                     "1. Write your answer as a concise, direct response."
                     "2. Keep your response brief and clear."
                     "3. If you cannot answer based on the context, reply exactly: 'ì œê°€ ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì…ë‹ˆë‹¤. ì—°êµ¬ì§„ì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.'"
          }]
        + history
        + [{"role": "user", "content": f"Context:\n{context_text}\n\nQuestion: {query}"}]
    )

    return client.chat.completions.create(
        model       = model,
        messages    = messages,
        stream      = True,
        max_tokens  = 512, # ê³ ë ¤
        temperature = 0,
    )
#===================================================================================
# ìˆ˜ì‹ í‘œí˜„ í•¨ìˆ˜ë“¤
#===================================================================================
# \[ $...$ \]ë¥¼ \[...\]ë¡œ ë³€í™˜
def clean_text(text):
    # ë¸”ë¡ ìˆ˜ì‹(\[ ... \]) ë‚´ë¶€ì˜ í•œ ê°œì§œë¦¬ ë°±ìŠ¬ë˜ì‹œë¥¼ ëª¨ë‘ ë‘ ê°œë¡œ
    def replacer(match):
        content = match.group(1)
        # ì´ë¯¸ \\ëŠ” ê±´ë“œë¦¬ì§€ ì•Šê³ , \ë§Œ ë‘ ê°œë¡œ!
        content_fixed = re.sub(r'\\([a-zA-Z]+)', r'\\\1', content)
        return f"\\[{content_fixed}\\]"
    return re.sub(r'\\\[(.*?)\\\]', replacer, text, flags=re.DOTALL)

# latex ìˆ˜ì‹ì—ì„œ í•œê¸€ì´ í¬í•¨ëœ ë¶„ìˆ˜ í‘œí˜„ì„ ì˜ˆì˜ê²Œ ê°€ê³µ
def enhance_korean_fraction(expr: str) -> str:
    # í•œê¸€ ë¬¸ìì—´ì„ \text{...}ë¡œ ê°ì‹¸ê¸°
    def wrap_korean(text: str):
        return re.sub(r"([ê°€-í£]+)", r"\\text{\1}", text)

    pattern = r"\\frac\s*{\s*(.+?)\s*}\s*{\s*(.+?)\s*}"

    def repl(match):
        numerator = wrap_korean(match.group(1)) # ë¶„ì
        denominator = wrap_korean(match.group(2)) # ë¶„ëª¨

        # displaystyle ë° ìˆ˜ì§ ì •ë ¬ ì¶”ê°€
        numerator = rf"\rule{{0pt}}{{1em}}{numerator}"
        denominator = rf"{denominator}\rule[-1em]{{0pt}}{{0pt}}"
        return rf"\displaystyle \frac{{{numerator}}}{{{denominator}}}"

    return re.sub(pattern, repl, expr)

# latexë¥¼ ì¸ë¼ì¸ ë˜ëŠ” ë¸”ë¡ìœ¼ë¡œ í‘œí˜„í• ì§€ íŒë‹¨
def display_with_latex(text):
    # ë¸”ë¡ ìˆ˜ì‹ êµ¬ê°„ split
    blocks = re.split(r'\\\[(.*?)\\\]', text, flags=re.DOTALL)
    for i, block in enumerate(blocks):
        if i % 2 == 0:
            # ì„¤ëª…ë¬¸, ì¸ë¼ì¸ í…ìŠ¤íŠ¸
            st.write(block)
        else:
            # LaTeX ë¸”ë¡ ìˆ˜ì‹
            enhanced = enhance_korean_fraction(block.strip())
            st.latex(enhanced)

#===================================================================================
# í˜ì´ì§€ì¸ì§€, ìŠˆë„ì½”ë“œ ì ˆì¸ì§€ë¥¼ í™•ì¸í•´ì„œ í•´ë‹¹ í•¨ìˆ˜ë¥¼ ë¶ˆëŸ¬ì£¼ëŠ” í•¨ìˆ˜
#===================================================================================
def query_by_question_subject_location_pseudo(query, question_subject):
    q_type = clean_phrase(query)

    if question_subject in ("location", "location_or_code"):
        return find_in_pages(q_type)
    elif question_subject == "code" or is_pseudocode(query) == "ìŠˆë„ì½”ë“œ":
        return find_pseudocode_sections(q_type)
    else:
        return None

#===================================================================================
# ìœ„ì¹˜ ì§ˆë¬¸ì— ëŒ€í•œ í•¨ìˆ˜
#===================================================================================
def find_in_pages(q_type):
    keywords_list = extract_nouns(q_type)
    n = len(keywords_list)
    answer_lines = []
    shown_phrases = set()  # ì´ë¯¸ í‘œì‹œí•œ í‘œê¸°(ëŒ€í‘œ í‘œê¸°, ë¶™ì—¬ì“°ê¸°/ë„ì–´ì“°ê¸° ëª¨ë‘)

    # 2ê°œ ì´ìƒ ë‹¨ì–´ë©´ ë³µí•©ì–´ ìš°ì„ !
    if n >= 2:
        phrase = " ".join(keywords_list)
        phrase_nospace = phrase.replace(" ", "")
        # ëŒ€í‘œ í‘œê¸°ëŠ” ë„ì–´ì“°ê¸° ìˆëŠ” ìª½ìœ¼ë¡œ!
        found = False
        for cand, display_phrase in [(phrase, phrase), (phrase_nospace, phrase)]:
            if display_phrase in shown_phrases:
                continue
            for label, meta_pages in PAGE_VOLUME_LIST:
                matched_pages = find_pages_with_keywords([cand], meta_pages)
                if matched_pages:
                    answer_lines.append(
                        f'**{display_phrase}**ì€(ëŠ”) **{label}** {", ".join(map(str, matched_pages))}ìª½(í˜ì´ì§€)ì— ë‚˜ì˜µë‹ˆë‹¤.\n'
                    )
                    shown_phrases.add(display_phrase)
                    found = True
        if found:
            return "\n".join(answer_lines)

    # ë³µí•©ì–´ë¡œ ëª» ì°¾ì•˜ì„ ë•Œë§Œ ë‹¨ì¼ì–´ë¡œ ê°ì ê²€ìƒ‰
    for k in keywords_list:
        if k in shown_phrases:
            continue
        for label, meta_pages in PAGE_VOLUME_LIST:
            matched_pages = find_pages_with_keywords([k], meta_pages)
            if matched_pages:
                answer_lines.append(
                    f'**{k}**ì€(ëŠ”) **{label}** {", ".join(map(str, matched_pages))}ìª½(í˜ì´ì§€)ì— ë‚˜ì˜µë‹ˆë‹¤.\n'
                )
                shown_phrases.add(k)
    return "\n".join(answer_lines) if answer_lines else "í•´ë‹¹ í˜ì´ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

#===================================================================================
# ìŠˆë„ì½”ë“œ ì§ˆë¬¸ì— ëŒ€í•œ í•¨ìˆ˜
#===================================================================================
def find_pseudocode_sections(q_type):
    keywords = extract_keywords(q_type)
    concept_keywords = [k for k in keywords if not is_pseudocode_keyword(k)]
    phrase = " ".join(concept_keywords)
    matched_sections = []

    for label, meta_keywords in SECTION_VOLUME_LIST:
        for meta_kw in meta_keywords:
            # ë³µí•©ì–´(ë„ì–´ì“°ê¸°/ë¶™ì—¬ì“°ê¸°) ëª¨ë‘ ê²€ì‚¬
            candidates = [phrase]
            if " " in phrase:
                candidates.append(phrase.replace(" ", ""))
            for cand in candidates:
                if any(cand in item for item in meta_kw["keywords"]):
                    if "ìŠˆë„ ì½”ë“œ" in meta_kw["text"] or "ìŠˆë„ì½”ë“œ" in meta_kw["text"]:
                        matched_sections.append((label, meta_kw))

    if matched_sections:
        answers = [
            f"**{label}** {section.get('section', 'í•´ë‹¹ ì ˆ')} ì ˆì— ìŠˆë„ì½”ë“œê°€ ìˆìŠµë‹ˆë‹¤."
            for label, section in matched_sections
        ]
        return "\n\n".join(answers)
    else:
        return "í•´ë‹¹ ì ˆì—ëŠ” ìŠˆë„ì½”ë“œê°€ ì—†ìŠµë‹ˆë‹¤."

#===================================================================================
# ì£¼ì–´ì§„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ì—ì„œ, 1~3ê°œì”© ë‹¨ì–´ë¥¼ ì—°ì†ìœ¼ë¡œ ë¬¶ì€ ëª¨ë“  phraseë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
#===================================================================================
def ngram_phrases(keywords_list, max_n=3):
    ngrams = []
    for n in range(max_n, 0, -1):
        for i in range(len(keywords_list) - n + 1):
            phrase = " ".join(keywords_list[i:i+n])
            ngrams.append(phrase)
    return ngrams

#===================================================================================
# í‚¤ì›Œë“œë¥¼ ì´ìš©í•˜ì—¬ í˜ì´ì§€ë¥¼ ì°¾ì•„ë‚´ëŠ” í•¨ìˆ˜
#===================================================================================
def find_pages_with_keywords(keywords, meta_pages):
    results = []
    if isinstance(keywords, str):
        keywords = [keywords]

    for page_meta in meta_pages:
        text = page_meta["text"]
        text_nospace = re.sub(r'\s+', '', text)  # ëª¨ë“  ê³µë°±ë¥˜ ì œê±°
        if all(
            k.replace(" ", "") in text_nospace
            for k in keywords
        ):
            results.append(page_meta["page"])
    return sorted(set(results))

#===================================================================================
# ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì¸ì§€ í™•ì¸í•˜ëŠ” í•¨ìˆ˜
#===================================================================================
def is_insufficient_answer(answer: str) -> bool:
    return (
        "ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì…ë‹ˆë‹¤. ì—°êµ¬ì§„ì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”" in answer
    )

#===================================================================================
# ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© í•¨ìˆ˜
#===================================================================================
def get_embedding(text):
    return _embed_text([text])[0]

#===================================================================================
# ë‘ ì„ë² ë”© ë²¡í„°ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜
#===================================================================================
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (norm(vec1) * norm(vec2))

#===================================================================================
# ìŠˆë„ì½”ë“œ ê´€ë ¨ í•¨ìˆ˜ë“¤
#===================================================================================
# ìŠˆë„ ì½”ë“œ, ìˆ˜ë„ì½”ë“œ ë“± -> ëª¨ë‘ "ìŠˆë„ì½”ë“œ"ë¡œ target ì •í•¨
def is_pseudocode(query: str, threshold=0.6) -> str | bool:
    target = "ìŠˆë„ì½”ë“œ"
    # target_vec = get_embedding(target)
    target_vec = get_embedding_cached("ìŠˆë„ì½”ë“œ")  # âœ… ìºì‹œ ì‚¬ìš©

    # ì˜ì–´ í‘œí˜„ì„ í•œê¸€ì‹ìœ¼ë¡œ ì¹˜í™˜
    normalized_query = query.lower().replace("pseudo", "ìŠˆë„").replace("code", "ì½”ë“œ")

    candidates = normalized_query.split(" ")
    for i in range(len(candidates)):
        for j in range(i + 1, min(len(candidates), i + 2)):
            phrase = " ".join(candidates[i:j+1])

            try:
                # vec = get_embedding(phrase)
                vec = get_embedding_cached(phrase)  # âœ… ìºì‹œ ì‚¬ìš©

                sim = cosine_similarity(vec, target_vec)
                if sim >= threshold:
                    return target
            except Exception as e:
                continue

    return False

# ì–´ë–¤ ìŠˆë„ì½”ë“œë¥¼ ì›í•˜ëŠ” ê²ƒì¸ì§€ ì£¼ìš” í‚¤ì›Œë“œë¥¼ ë½‘ì•„ë‚´ëŠ” í•¨ìˆ˜
def is_pseudocode_keyword(word: str, threshold=0.4) -> bool:
    # ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ë°˜ìœ¼ë¡œ íŒë³„
    target = "ìŠˆë„ì½”ë“œ"
    word = word.lower().replace("pseudo", "ìŠˆë„").replace("code", "ì½”ë“œ")
    target_vec = get_embedding(target)
    word_vec = get_embedding(word)

    sim = cosine_similarity(word_vec, target_vec)
    return sim >= threshold

#===================================================================================
# ì¡°ì‚¬ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜
#===================================================================================
def clean_phrase(phrase):
    # "ì˜", "ê°€", "ì„", "ë¥¼" ë“±ì˜ ì¡°ì‚¬ë¥¼ ëª¨ë‘ ì œê±°
    return re.sub(r'(ì˜|ê°€ |ì„|ë¥¼|ì€|ëŠ”|ì´ |ì—|ì™€|ê³¼|ë¡œ|ìœ¼ë¡œ|,)', ' ', phrase)

#===================================================================================
# 2ê¸€ì ì´ìƒ í•œê¸€, ì—°ì† ì¶”ì¶œ í•¨ìˆ˜
#===================================================================================
def extract_nouns(text):
    # ê¸°ì¡´: ëª¨ë“  2ê¸€ì ì´ìƒ í•œê¸€ ì¶”ì¶œ
    words = re.findall(r'[ê°€-í£]{2,}', text)
    # ë¶ˆìš©ì–´ ì œê±°
    return [w for w in words if w not in STOPWORDS]

#===================================================================================
# ì–´ë–¤ ëª¨ë¸ë¡œ ë‹µë³€í–ˆëŠ”ì§€ ì²´í¬ í•¨ìˆ˜
#===================================================================================
def contains_model_tag(text, model):
    # ëª¨ë¸ëª… ë¬¸êµ¬ê°€ ì´ë¯¸ í¬í•¨ëëŠ”ì§€ ì²´í¬
    tag = f"{model}ë¡œ ë‹µë³€"
    tag2 = f"**{model}**ë¡œ ë‹µë³€"
    return (tag in text) or (tag2 in text)

#===================================================================================
# í† í° ì œí•œ í•¨ìˆ˜ë“¤
#===================================================================================
# í† í° ê³„ì‚°
def count_tokens(text):
    enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(enc.encode(text))

# ìƒˆ ì§ˆë¬¸ ì²˜ë¦¬
def handle_question(prompt):
    prompt_tokens = count_tokens(prompt)
    current = int(r.get(key) or 0)

    if current + prompt_tokens > TOKEN_LIMIT:
        return "ì˜¤ëŠ˜ì˜ í† í° ì‚¬ìš©ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."

    # í† í° ì¦ê°€
    r.incrby(key, prompt_tokens)
    return "ì§ˆë¬¸ ì²˜ë¦¬ ì™„ë£Œ"

# í† í° ì‚¬ìš©ëŸ‰ í™•ì¸
def get_token_usage(user_id):
    today = datetime.now().strftime("%Y-%m-%d")
    key = f"tokens:{user_id}:{today}"
    return int(r.get(key) or 0)

#===================================================================================
# ìºì‹œ ì €ì¥
#===================================================================================
cache = {}
def get_embedding_cached(text):
    if text in cache:
        return cache[text]  # ğŸ‘‰ ì €ì¥ëœ ê°’ ì¬ì‚¬ìš©
    emb = get_embedding(text)
    cache[text] = emb       # ğŸ‘‰ ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
    return emb

#===================================================================================
# Streamlit UI
#===================================================================================
CHUNKS_VOLUME_LIST = build_or_load()
today = datetime.now().strftime("%Y-%m-%d")
key = f"tokens:{USER_ID}:{today}"

st.set_page_config(page_title="LYDUS Chatbot")
st.title("ğŸ–¥ï¸ LYDUS Chatbot")
st.error("ì´ ì±—ë´‡ì€ ì°¸ê³ ìš©ìœ¼ë¡œ ì œê³µë˜ë©°, ì¤‘ìš”í•œ ë‚´ìš©ì€ ë°˜ë“œì‹œ ê³µì‹ ê°€ì´ë“œë¼ì¸ì„ í™•ì¸í•˜ì„¸ìš”.")

# ì„¸ì…˜ ê°„ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ìœ ì§€í•˜ê¸° ìœ„í•œ ì´ˆê¸°í™” ì½”ë“œ
if "history" not in st.session_state:
    st.session_state.history = []

# ê³¼ê±° ëŒ€í™” ì „ì²´ ì¶œë ¥
for h in st.session_state.history:
    with st.chat_message(h["role"]):
        display_with_latex(h["content"])

# ìƒˆ ì§ˆë¬¸ ì…ë ¥ë°›ê¸°
if prompt := st.chat_input("ê°€ì´ë“œë¼ì¸ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”â€¦"):
    # í† í°
    result = handle_question(prompt)
    usage = get_token_usage(USER_ID)
    st.markdown(f"ì˜¤ëŠ˜ ì‚¬ìš©í•œ í† í° ìˆ˜: **{usage} / {TOKEN_LIMIT}**")

    # 3. user ì§ˆë¬¸ ì¦‰ì‹œ ì¶œë ¥
    st.chat_message("user").markdown(prompt)
    st.session_state.history.append({"role": "user", "content": prompt})

    with st.chat_message("assistant"):
        if "í† í° ì‚¬ìš©ëŸ‰ì„ ì´ˆê³¼" in result:
            st.markdown(f"{result}")
            st.stop()

        question = classify_question(prompt)
        if question == "other":
            full_response = ""
            for chunk in rag_chat_multi_volume(prompt, st.session_state.history):
                delta = chunk.choices[0].delta.content or ""
                full_response += delta

            if is_insufficient_answer(full_response):
                full_response = ""
                for chunk in rag_chat_multi_volume(prompt, st.session_state.history, model=CHAT_MODEL_GENERAL):
                    delta = chunk.choices[0].delta.content or ""
                    full_response += delta
                if not contains_model_tag(full_response, CHAT_MODEL_GENERAL):
                    full_response += f"\n\n**{CHAT_MODEL_GENERAL}**ë¡œ ë‹µë³€"
            else:
                if not contains_model_tag(full_response, CHAT_MODEL_MINI):
                    full_response += f"\n\n**{CHAT_MODEL_MINI}**ë¡œ ë‹µë³€"

            display_with_latex(full_response)
            st.session_state.history.append({
                "role": "assistant",
                "content": full_response
            })

            st.stop()

        # ìœ„ì¹˜, ìŠˆë„ì½”ë“œë¥¼ ë¬¼ì–´ë³´ëŠ” ê²½ìš°
        answer = query_by_question_subject_location_pseudo(prompt, question)
        if answer:
            display_with_latex(answer)
            st.session_state.history.append({
                "role": "assistant",
                "content": answer
            })
            st.stop()